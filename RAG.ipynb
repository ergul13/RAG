{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1DQfvVpRZYndO3zxjeneJzDf3jjU5I3S2",
      "authorship_tag": "ABX9TyN7PNUj9dbRSVGk2mmYHzZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ergul13/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess\n",
        "py = sys.executable\n",
        "def pipi(x): subprocess.check_call([py, \"-m\", \"pip\", \"install\", \"--quiet\"] + x.split())\n",
        "\n",
        "pipi(\"torch --index-url https://download.pytorch.org/whl/cu121\")\n",
        "pipi(\"transformers accelerate bitsandbytes sentencepiece\")\n",
        "pipi(\"faiss-cpu langdetect rank_bm25 pypdf beautifulsoup4 lxml\")\n",
        "pipi(\"FlagEmbedding\")\n",
        "pipi(\"gradio\")"
      ],
      "metadata": {
        "id": "iuDwjogFxZ6S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, glob, zipfile, pathlib\n",
        "BASE_DIR = \"/content/rag_suite\"\n",
        "DATA_DIR = f\"{BASE_DIR}/data/raw\"\n",
        "PROC_DIR = f\"{BASE_DIR}/data/processed\"\n",
        "INDEX_DIR = f\"{BASE_DIR}/data/index\"\n",
        "FAISS_DIR = f\"{INDEX_DIR}/faiss\"\n",
        "BM25_DIR = f\"{INDEX_DIR}/bm25\"\n",
        "\n",
        "for p in [DATA_DIR, PROC_DIR, FAISS_DIR, BM25_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "if os.path.exists(\"/content/Rag.zip\"):\n",
        "    with zipfile.ZipFile(\"/content/Rag.zip\",\"r\") as z: z.extractall(DATA_DIR)\n",
        "\n",
        "# İstersen dosya yüklemek için:\n",
        "# from google.colab import files\n",
        "# files.upload()  # örn. data.zip seç\n",
        "# !unzip -o data.zip -d /content/rag_suite/data/raw"
      ],
      "metadata": {
        "id": "UxyiQq22xaky"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, gc, json, pickle, glob, hashlib, shutil, time\n",
        "from typing import List, Tuple, Dict\n",
        "import torch, faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from FlagEmbedding import BGEM3FlagModel, FlagReranker\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langdetect import detect\n",
        "from pypdf import PdfReader\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "CFG = {\n",
        "    \"LLM_MODEL_QWEN\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    \"LLM_MODEL_LLAMA\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"USE_LLAMA\": False,\n",
        "    \"LOAD_4BIT\": True,\n",
        "    \"MAX_NEW_TOKENS\": 512,\n",
        "    \"EMB_MODEL\": \"BAAI/bge-m3\",\n",
        "    \"RERANK_MODEL\": \"BAAI/bge-reranker-v2-m3\",\n",
        "    \"CHUNK_TOK_MIN\": 200,\n",
        "    \"CHUNK_TOK_MAX\": 600,\n",
        "    \"CHUNK_OVERLAP_TOK\": 80,\n",
        "    \"TOPK_DENSE\": 30,\n",
        "    \"TOPK_BM25\": 30,\n",
        "    \"MERGE_TOPK\": 50,\n",
        "    \"FINAL_CONTEXTS\": 8,\n",
        "    \"RRF_K\": 60,\n",
        "    \"MQ_COUNT\": 3,\n",
        "    \"CTX_TOKEN_LIMIT\": 2800,\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "\n",
        "torch.manual_seed(CFG[\"SEED\"])\n",
        "\n",
        "def read_txt(p):\n",
        "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def read_pdf(p):\n",
        "    pdf = PdfReader(p)\n",
        "    return \"\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "\n",
        "def read_html(p):\n",
        "    html = read_txt(p)\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    for s in soup([\"script\",\"style\",\"noscript\"]): s.extract()\n",
        "    return soup.get_text(\"\\n\")\n",
        "\n",
        "def read_md(p):\n",
        "    return read_txt(p)\n",
        "\n",
        "def read_code(p):\n",
        "    return read_txt(p)\n",
        "\n",
        "def detect_lang_safe(text):\n",
        "    try: return detect(text[:1000])\n",
        "    except: return \"unknown\"\n",
        "\n",
        "def tokenize_for_bm25(text):\n",
        "    return re.findall(r\"\\w+\", text.lower())\n",
        "\n",
        "def simple_token_count(s):\n",
        "    return max(1, len(re.findall(r\"\\S+\", s)))\n",
        "\n",
        "def split_chunks(text, min_tok, max_tok, overlap_tok):\n",
        "    toks = re.findall(r\"\\S+\", text)\n",
        "    chunks, i = [], 0\n",
        "    while i < len(toks):\n",
        "        size = max_tok\n",
        "        chunk = toks[i:i+size]\n",
        "        if len(chunk) < min_tok and i != 0: break\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += max(1, size - overlap_tok)\n",
        "    return chunks\n",
        "\n",
        "def smart_chunk(file_path, text):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext in [\".py\",\".js\",\".ts\",\".java\",\".cpp\",\".c\",\".ipynb\",\".rs\",\".go\",\".rb\",\".php\",\".cs\",\".scala\",\".kt\",\".swift\"]:\n",
        "        lines = text.splitlines()\n",
        "        blocks, buf = [], []\n",
        "        for line in lines:\n",
        "            buf.append(line)\n",
        "            if re.match(r\"^\\s*(def |class |function |if |for |while |switch|public |private |protected |#[^!]|@)\", line) and len(buf) > 30:\n",
        "                blocks.append(\"\\n\".join(buf)); buf=[]\n",
        "        if buf: blocks.append(\"\\n\".join(buf))\n",
        "        text_blocks = [b for blk in blocks for b in split_chunks(blk, CFG[\"CHUNK_TOK_MIN\"]//2, CFG[\"CHUNK_TOK_MAX\"], CFG[\"CHUNK_OVERLAP_TOK\"]//2)]\n",
        "        if not text_blocks:\n",
        "            text_blocks = split_chunks(text, CFG[\"CHUNK_TOK_MIN\"], CFG[\"CHUNK_TOK_MAX\"], CFG[\"CHUNK_OVERLAP_TOK\"])\n",
        "        return text_blocks\n",
        "    if ext == \".md\":\n",
        "        parts = re.split(r\"\\n(?=#)\", text)\n",
        "        out=[]\n",
        "        for part in parts:\n",
        "            out += split_chunks(part, CFG[\"CHUNK_TOK_MIN\"], CFG[\"CHUNK_TOK_MAX\"], CFG[\"CHUNK_OVERLAP_TOK\"])\n",
        "        return out\n",
        "    return split_chunks(text, CFG[\"CHUNK_TOK_MIN\"], CFG[\"CHUNK_TOK_MAX\"], CFG[\"CHUNK_OVERLAP_TOK\"])\n",
        "\n",
        "def load_file(path):\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext==\".pdf\": raw = read_pdf(path)\n",
        "    elif ext in [\".html\",\".htm\"]: raw = read_html(path)\n",
        "    elif ext==\".md\": raw = read_md(path)\n",
        "    elif ext in [\".txt\",\".log\",\".csv\",\".tsv\",\".json\",\".yml\",\".yaml\"]: raw = read_txt(path)\n",
        "    elif ext in [\".py\",\".js\",\".ts\",\".java\",\".cpp\",\".c\",\".ipynb\",\".rs\",\".go\",\".rb\",\".php\",\".cs\",\".scala\",\".kt\",\".swift\"]: raw = read_code(path)\n",
        "    else: raw = \"\"\n",
        "    return raw\n",
        "\n",
        "def hash_str(s): return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
        "\n",
        "def build_index(input_files: List[str]):\n",
        "    model = BGEM3FlagModel(CFG[\"EMB_MODEL\"], use_fp16=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    faiss_path = f\"{FAISS_DIR}/index.faiss\"\n",
        "    meta_path = f\"{FAISS_DIR}/meta.pkl\"\n",
        "    bm25_path = f\"{BM25_DIR}/bm25.pkl\"\n",
        "\n",
        "    corpus_tokens = []\n",
        "    corpus_texts = []\n",
        "    corpus_meta = []\n",
        "    dense_vectors = []\n",
        "\n",
        "    for fp in input_files:\n",
        "        raw = load_file(fp)\n",
        "        if not raw.strip(): continue\n",
        "        lang = detect_lang_safe(raw)\n",
        "        chunks = smart_chunk(fp, raw)\n",
        "        for idx, ch in enumerate(chunks):\n",
        "            if not ch.strip(): continue\n",
        "            doc_id = f\"{hash_str(fp)}_{idx}\"\n",
        "            corpus_texts.append(ch)\n",
        "            corpus_meta.append({\"id\": doc_id, \"source\": fp, \"lang\": lang})\n",
        "            corpus_tokens.append(tokenize_for_bm25(ch))\n",
        "            dense = model.encode(ch, batch_size=1)[\"dense_vecs\"][0]\n",
        "            dense_vectors.append(dense.astype(\"float32\"))\n",
        "\n",
        "    if not corpus_texts:\n",
        "        raise RuntimeError(\"No text extracted. Upload supported files.\")\n",
        "\n",
        "    xb = torch.stack([torch.from_numpy(v) for v in dense_vectors]).cpu().numpy()\n",
        "    index = faiss.IndexFlatIP(xb.shape[1])\n",
        "    faiss.normalize_L2(xb)\n",
        "    index.add(xb)\n",
        "    faiss.write_index(index, faiss_path)\n",
        "\n",
        "    with open(meta_path, \"wb\") as f: pickle.dump({\"texts\": corpus_texts, \"meta\": corpus_meta}, f)\n",
        "    bm25 = BM25Okapi(corpus_tokens)\n",
        "    with open(bm25_path, \"wb\") as f: pickle.dump({\"bm25\": bm25, \"texts\": corpus_texts, \"meta\": corpus_meta}, f)\n",
        "\n",
        "def load_indices():\n",
        "    faiss_path = f\"{FAISS_DIR}/index.faiss\"\n",
        "    meta_path = f\"{FAISS_DIR}/meta.pkl\"\n",
        "    bm25_path = f\"{BM25_DIR}/bm25.pkl\"\n",
        "    index = faiss.read_index(faiss_path)\n",
        "    with open(meta_path, \"rb\") as f: mm = pickle.load(f)\n",
        "    with open(bm25_path, \"rb\") as f: bm = pickle.load(f)\n",
        "    return index, mm[\"texts\"], mm[\"meta\"], bm[\"bm25\"]\n",
        "\n",
        "def dense_search(emb_model, index, texts, query, topk):\n",
        "    q_vec = emb_model.encode(query, batch_size=1)[\"dense_vecs\"][0].astype(\"float32\").reshape(1, -1)\n",
        "    faiss.normalize_L2(q_vec)\n",
        "    D, I = index.search(q_vec, topk)\n",
        "    res = []\n",
        "    for d, i in zip(D[0], I[0]):\n",
        "        if i < 0: continue\n",
        "        res.append((int(i), float(d)))\n",
        "    return res\n",
        "\n",
        "def bm25_search(bm25, query, topk):\n",
        "    toks = re.findall(r\"\\w+\", query.lower())\n",
        "    scores = bm25.get_scores(toks)\n",
        "    idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:topk]\n",
        "    return [(i, float(scores[i])) for i in idx]\n",
        "\n",
        "def rrf_fuse(ranklists: List[List[Tuple[int,float]]], k=60, topk=50):\n",
        "    scores = {}\n",
        "    for rl in ranklists:\n",
        "        for r, (i, _) in enumerate(rl):\n",
        "            scores[i] = scores.get(i, 0.0) + 1.0 / (k + r + 1)\n",
        "    fused = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
        "    return fused\n",
        "\n",
        "def load_llm():\n",
        "    model_id = CFG[\"LLM_MODEL_LLAMA\"] if CFG[\"USE_LLAMA\"] else CFG[\"LLM_MODEL_QWEN\"]\n",
        "    kwargs = {}\n",
        "    if CFG[\"LOAD_4BIT\"]:\n",
        "        kwargs.update(dict(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, device_map=\"auto\"))\n",
        "    else:\n",
        "        kwargs.update(dict(torch_dtype=torch.float16, device_map=\"auto\"))\n",
        "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, **kwargs)\n",
        "    return tok, mdl\n",
        "\n",
        "def load_reranker():\n",
        "    return FlagReranker(CFG[\"RERANK_MODEL\"], use_fp16=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def llm_generate(tok, mdl, prompt, temperature=0.2, max_new_tokens=CFG[\"MAX_NEW_TOKENS\"]):\n",
        "    ids = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    out = mdl.generate(**ids, do_sample=True, temperature=temperature, max_new_tokens=max_new_tokens, eos_token_id=tok.eos_token_id)\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def multi_query_variants(tok, mdl, q, n=3, lang=\"en\"):\n",
        "    sysmsg = \"You generate diverse rephrasings of a user query for better retrieval. Return each variant on a new line without numbering.\"\n",
        "    if lang == \"tr\":\n",
        "        sysmsg = \"Kullanıcı sorgusunu daha iyi arama için çeşitli yeniden ifadeler üret. Numaralandırma yapmadan her varyantı yeni satırda döndür.\"\n",
        "    prompt = f\"<|system|>\\n{sysmsg}\\n<|user|>\\n{q}\\n\"\n",
        "    raw = llm_generate(tok, mdl, prompt, temperature=0.7, max_new_tokens=128)\n",
        "    lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
        "    uniq, seen = [], set()\n",
        "    for l in lines:\n",
        "        if l.lower().startswith((\"assistant\",\"system\",\"user\")): continue\n",
        "        if l not in seen:\n",
        "            uniq.append(l); seen.add(l)\n",
        "        if len(uniq) >= n: break\n",
        "    if not uniq: uniq = [q, q+\"?\", q+\" details\"]\n",
        "    return uniq\n",
        "\n",
        "def truncate_contexts(ctxs: List[str], limit_tokens=2800):\n",
        "    out, count = [], 0\n",
        "    for c in ctxs:\n",
        "        tc = simple_token_count(c)\n",
        "        if count + tc > limit_tokens: break\n",
        "        out.append(c); count += tc\n",
        "    return out\n",
        "\n",
        "def build_prompt(query, contexts, metas, lang=\"en\"):\n",
        "    citations = []\n",
        "    for i,(c,m) in enumerate(zip(contexts, metas), start=1):\n",
        "        src = m.get(\"source\",\"\")\n",
        "        citations.append(f\"[{i}] {os.path.basename(src)} | {src}\")\n",
        "    ctx_block = \"\\n\\n\".join([f\"[{i+1}] {c}\" for i,c in enumerate(contexts)])\n",
        "    if lang==\"tr\":\n",
        "        instr = \"Aşağıdaki bağlam parçalarına dayanarak soruyu yanıtla. Yanıtın sonunda kullandığın kaynak numaralarını belirt. Bağlamda yoksa 'yetersiz bağlam' de.\"\n",
        "    else:\n",
        "        instr = \"Answer the question based strictly on the context. Cite the used source numbers at the end. If information is missing, say 'insufficient context'.\"\n",
        "    prompt = f\"<|system|>\\n{instr}\\n<|user|>\\nQuestion:\\n{query}\\n\\nContext:\\n{ctx_block}\\n\\nAnswer:\\n\"\n",
        "    return prompt, \"\\n\".join(citations)\n",
        "\n",
        "def rerank_and_select(reranker, query, texts, metas, n_final):\n",
        "    pairs = [(query, t) for t in texts]\n",
        "    scores = reranker.compute_score(pairs, normalize=True)\n",
        "    ranked = list(zip(range(len(texts)), scores))\n",
        "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
        "    idx = [i for i,_ in ranked[:n_final]]\n",
        "    sel_texts = [texts[i] for i in idx]\n",
        "    sel_metas = [metas[i] for i in idx]\n",
        "    return sel_texts, sel_metas\n",
        "\n",
        "def list_supported():\n",
        "    return [\".pdf\",\".html\",\".htm\",\".md\",\".txt\",\".log\",\".csv\",\".tsv\",\".json\",\".yml\",\".yaml\",\".py\",\".js\",\".ts\",\".java\",\".cpp\",\".c\",\".ipynb\",\".rs\",\".go\",\".rb\",\".php\",\".cs\",\".scala\",\".kt\",\".swift\"]\n",
        "\n",
        "def build_all_indices():\n",
        "    files = []\n",
        "    for ext in list_supported():\n",
        "        files += glob.glob(os.path.join(DATA_DIR, f\"*{ext}\"))\n",
        "    if not files:\n",
        "        raise RuntimeError(\"Upload some files first.\")\n",
        "    build_index(files)\n",
        "    return f\"Indexed {len(files)} files.\"\n",
        "\n",
        "def run_retrieval(query, lang_hint=None):\n",
        "    index, texts, meta, bm25 = load_indices()\n",
        "    emb = BGEM3FlagModel(CFG[\"EMB_MODEL\"], use_fp16=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tok, mdl = load_llm()\n",
        "    lang = lang_hint or (\"tr\" if any(ch in query for ch in \"ığüşöçİĞÜŞÖÇ\") else \"en\")\n",
        "    variants = [query] + multi_query_variants(tok, mdl, query, n=CFG[\"MQ_COUNT\"], lang=lang)\n",
        "    dense_lists = [dense_search(emb, index, texts, v, CFG[\"TOPK_DENSE\"]) for v in variants]\n",
        "    bm25_lists = [bm25_search(bm25, v, CFG[\"TOPK_BM25\"]) for v in variants]\n",
        "    fused = rrf_fuse(dense_lists + bm25_lists, k=CFG[\"RRF_K\"], topk=CFG[\"MERGE_TOPK\"])\n",
        "    cand_idx = [i for i,_ in fused]\n",
        "    cand_texts = [texts[i] for i in cand_idx]\n",
        "    cand_metas = [meta[i] for i in cand_idx]\n",
        "    rr = load_reranker()\n",
        "    sel_texts, sel_metas = rerank_and_select(rr, query, cand_texts, cand_metas, CFG[\"FINAL_CONTEXTS\"])\n",
        "    return sel_texts, sel_metas, tok, mdl, lang\n",
        "\n",
        "def answer_query(query, lang_hint=None):\n",
        "    ctx_texts, ctx_metas, tok, mdl, lang = run_retrieval(query, lang_hint)\n",
        "    if not ctx_texts: return \"No context found.\", []\n",
        "    ctx_texts = truncate_contexts(ctx_texts, CFG[\"CTX_TOKEN_LIMIT\"])\n",
        "    prompt, cite_list = build_prompt(query, ctx_texts, ctx_metas, lang)\n",
        "    ans = llm_generate(tok, mdl, prompt, temperature=0.2, max_new_tokens=CFG[\"MAX_NEW_TOKENS\"])\n",
        "    return ans, ctx_metas"
      ],
      "metadata": {
        "id": "FigDNg0txf0v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def ui_ingest(files):\n",
        "    saved = []\n",
        "    if files:\n",
        "        for f in files:\n",
        "            dst = os.path.join(DATA_DIR, os.path.basename(f.name))\n",
        "            with open(dst, \"wb\") as w: w.write(f.read())\n",
        "            saved.append(dst)\n",
        "    msg = build_all_indices()\n",
        "    return f\"Uploaded {len(saved)} files. {msg}\"\n",
        "\n",
        "def ui_query(q):\n",
        "    if not os.path.exists(f\"{FAISS_DIR}/index.faiss\"):\n",
        "        raise gr.Error(\"Index not found. Upload and Build index first.\")\n",
        "    ans, metas = answer_query(q)\n",
        "    cites = \"\\n\".join([f\"- {os.path.basename(m['source'])} ({m['source']})\" for m in metas])\n",
        "    return ans, cites\n",
        "\n",
        "def set_model(choice):\n",
        "    CFG[\"USE_LLAMA\"] = (choice == \"Llama-3.1-8B-Instruct\")\n",
        "    return f\"Model set to: {choice}\"\n",
        "\n",
        "with gr.Blocks(title=\"Hybrid RAG Suite\") as demo:\n",
        "    gr.Markdown(\"# Hybrid RAG Suite (TR/EN) • Dense+BM25 • Rerank • Multi-Query • Qwen/Llama • Gradio\")\n",
        "\n",
        "    with gr.Tab(\"Ingest\"):\n",
        "        up = gr.File(file_count=\"multiple\", label=\"Upload files\")\n",
        "        btn_idx = gr.Button(\"Build/Refresh Index\")\n",
        "        out_idx = gr.Markdown()\n",
        "        up.change(ui_ingest, inputs=up, outputs=out_idx)\n",
        "        btn_idx.click(lambda: build_all_indices(), outputs=out_idx)\n",
        "        gr.Markdown(\"Supported: \" + \", \".join(list_supported()))\n",
        "\n",
        "    with gr.Tab(\"Chat\"):\n",
        "        model_choice = gr.Radio([\"Qwen2.5-7B-Instruct\",\"Llama-3.1-8B-Instruct\"], value=\"Qwen2.5-7B-Instruct\", label=\"Response LLM\")\n",
        "        setbtn = gr.Button(\"Set Model\")\n",
        "        setout = gr.Markdown()\n",
        "        setbtn.click(lambda c: set_model(c), inputs=model_choice, outputs=setout)\n",
        "        q = gr.Textbox(label=\"Query\")\n",
        "        ask = gr.Button(\"Ask\")\n",
        "        a = gr.Markdown()\n",
        "        cites = gr.Markdown()\n",
        "        ask.click(ui_query, inputs=q, outputs=[a, cites])\n",
        "\n",
        "    with gr.Tab(\"Utils\"):\n",
        "        btn_clean = gr.Button(\"Reset All\")\n",
        "        msg = gr.Markdown()\n",
        "        def reset_all():\n",
        "            shutil.rmtree(BASE_DIR, ignore_errors=True)\n",
        "            os.makedirs(DATA_DIR, exist_ok=True)\n",
        "            os.makedirs(PROC_DIR, exist_ok=True)\n",
        "            os.makedirs(FAISS_DIR, exist_ok=True)\n",
        "            os.makedirs(BM25_DIR, exist_ok=True)\n",
        "            return \"Reset done.\"\n",
        "        btn_clean.click(reset_all, outputs=msg)\n",
        "\n",
        "demo.launch(debug=False, share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "VHIv9RtZxj3k",
        "outputId": "8deeb96c-1849-40df-ef2a-cb729a2b9478"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOSXUyE6xnDd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}